{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "# Adding MaskedColumn - AEE\n",
    "from astropy.table import Table, vstack, Column, MaskedColumn\n",
    "# import argparse\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "import re\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "from desimeter.util import parse_fibers\n",
    "from desimeter.dbutil import dbquery,get_petal_ids,get_pos_ids,get_petal_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "date_to_analyze = 20210605\n",
    "write_to_file = False\n",
    "write_path = '/n/home/msdos/ann'\n",
    "write_to_db = True\n",
    "def output_to_file(the_table, output_filename, format='ascii.csv'):\n",
    "    if format == 'ascii.csv':\n",
    "        the_table.write(os.path.join(write_path, output_filename), format=format, overwrite=True)\n",
    "    elif format == 'pandas.json':\n",
    "        the_table.write(os.path.join(write_path, output_filename), format=format, indent=4, orient='records')\n",
    "    else:\n",
    "        the_table.write(os.path.join(write_path, output_filename), format=format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Commented this out - AEE\n",
    "#import matplotlib.pyplot as plt\n",
    "#%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The petal DB tables are by index number, but we refer to fibers by location\n",
    "petal_id2loc = {4:0, 5:1, 6:2, 3:3, 8:4, 10:5, 11:6, 2:7, 7:8, 9:9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'host': '', 'port': 1234, 'database': '', 'user': '', 'password': ''}\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "args = {'host': os.getenv('DOS_DB_HOST'), 'port': int(os.getenv('DOS_DB_PORT')), 'database': os.getenv('DOS_DB_NAME'), 'user': os.getenv('DOS_DB_READER'), 'password': os.getenv('DOS_DB_READER_PASSWORD')}\n",
    "print(args)\n",
    "\n",
    "comm = psycopg2.connect(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch a positioner's worth of data\n",
    "# cmd = \"select * from posmovedb.positioner_moves where (pos_id='M04866') and (time_recorded > '2021-05-17' ) order by time_recorded desc\"\n",
    "# db=dbquery(comm,cmd)\n",
    "# db = Table(db)\n",
    "# print(len(db), \"entries retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch a petals's worth of data\n",
    "# To select the data from a night, use the date of the next day.  So May 17 becomes 2021-05-18.\n",
    "# cmd = \"select * from posmovedb.positioner_moves_p8 where (time_recorded > '2021-05-19') order by time_recorded desc\"\n",
    "# db=dbquery(comm,cmd)\n",
    "# db = Table(db)\n",
    "# print(len(db), \"entries retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch all petals's worth of data\n",
    "# To select the data from a night, use the date of the next day.  So May 17 becomes 2021-05-18.\n",
    "# TODO: Need to make the date handling better\n",
    "def fetchDB(night):\n",
    "    _night = f'{night:08d}'\n",
    "    _night = date(int(_night[0:4]),int(_night[4:6]),int(_night[6:8]))\n",
    "    today = _night+timedelta(days=1)\n",
    "    tomorrow = _night+timedelta(days=2)\n",
    "    print(\"Searching DB from\", today.isoformat(), 'to', tomorrow.isoformat())\n",
    "    dbs = {}\n",
    "    for petal in petal_id2loc.keys():\n",
    "        print(\"Fetching petal\", petal)\n",
    "        cmd = \"select * from posmovedb.positioner_moves_p\"+f'{petal:1d}'+\" where (time_recorded > '\"+today.isoformat()+\"') and (time_recorded < '\"+tomorrow.isoformat()+\"') order by time_recorded desc\"\n",
    "    #    print(cmd)\n",
    "        db=dbquery(comm,cmd)\n",
    "        db = Table(db)\n",
    "        dbs[petal]=db\n",
    "        print(len(db), \"entries retrieved\")\n",
    "    print(\"All petals retrieved.\")\n",
    "    return dbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching DB from 2021-06-06 to 2021-06-07\n",
      "Fetching petal 4\n",
      "21328 entries retrieved\n",
      "Fetching petal 5\n",
      "21405 entries retrieved\n",
      "Fetching petal 6\n",
      "21005 entries retrieved\n",
      "Fetching petal 3\n",
      "21143 entries retrieved\n",
      "Fetching petal 8\n",
      "21201 entries retrieved\n",
      "Fetching petal 10\n",
      "21189 entries retrieved\n",
      "Fetching petal 11\n",
      "21000 entries retrieved\n",
      "Fetching petal 2\n",
      "21212 entries retrieved\n",
      "Fetching petal 7\n",
      "21644 entries retrieved\n",
      "Fetching petal 9\n",
      "21666 entries retrieved\n",
      "All petals retrieved.\n"
     ]
    }
   ],
   "source": [
    "dbs = fetchDB(date_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('petal_id', '<i8'), ('device_loc', '<i8'), ('pos_id', '<U6'), ('pos_move_index', '<i8'), ('time_recorded', 'O'), ('bus_id', '<U5'), ('pos_t', '<f8'), ('pos_p', '<f8'), ('last_meas_obs_x', 'O'), ('last_meas_obs_y', 'O'), ('last_meas_peak', 'O'), ('last_meas_fwhm', 'O'), ('total_move_sequences', '<i8'), ('total_cruise_moves_t', '<i8'), ('total_cruise_moves_p', '<i8'), ('total_creep_moves_t', '<i8'), ('total_creep_moves_p', '<i8'), ('ctrl_enabled', '?'), ('move_cmd', '<U85'), ('move_val1', '<U129'), ('move_val2', '<U126'), ('log_note', '<U206'), ('exposure_id', 'O'), ('exposure_iter', 'O'), ('flags', 'O'), ('obs_x', 'O'), ('obs_y', 'O'), ('ptl_x', 'O'), ('ptl_y', 'O'), ('ptl_z', 'O'), ('site', '<U4'), ('postscript', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(dbs[3].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the main analysis code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_fiber(db, quiet=False, showlog=False):\n",
    "    # We have to parse some columns that should be integers, but aren't in the raw table.\n",
    "    def parse_int(string):\n",
    "        if string == None: return 0\n",
    "        else: return int(string)\n",
    "    def parse_float(string):\n",
    "        if string == None: return 0\n",
    "        else: return float(string)\n",
    "    exposure = np.array(list(map(parse_int,db['exposure_id'])))\n",
    "    flags = np.array(list(map(parse_int,db['flags'])))\n",
    "    db_iter = np.array(list(map(parse_int,db['exposure_iter'])))\n",
    "    # These two lines are new - AEE\n",
    "    obs_x = np.array(list(map(parse_float,db['obs_x'])))\n",
    "    obs_y = np.array(list(map(parse_float,db['obs_y'])))\n",
    "\n",
    "\n",
    "    # A move was commanded\n",
    "    move_attempted = db['ctrl_enabled'] & np.array(list(map(lambda s: (s.find('QS')>=0), db['move_cmd'])))\n",
    "    # Which was supposed to be these computed angular distances:\n",
    "    def sum_move(string):\n",
    "        return np.sum(np.array(list(map(float, re.split('; | ',string)[1::2]))))\n",
    "    move_t = np.array(list(map(sum_move, db['move_val1'])))\n",
    "    move_p = np.array(list(map(sum_move, db['move_val2'])))\n",
    "    # We care about big moves, more than 10 degrees.\n",
    "    bigmove_t = move_attempted & (np.abs(move_t)>10)\n",
    "    bigmove_p = move_attempted & (np.abs(move_p)>10)\n",
    "    bigmove = bigmove_t|bigmove_p\n",
    "    \n",
    "    # Look for the commanded position\n",
    "    re_prog = re.compile('req_posintTP=\\(([\\d\\.-]*), ([\\d\\.-]*)\\)')\n",
    "    def parse_move(log):\n",
    "        match = re_prog.search(log)\n",
    "        if match: return np.array(list(map(float,match.groups())))\n",
    "        else: return np.array([999.0,999.0])\n",
    "    requested_moves = np.array(list(map(parse_move, db['log_note'])))\n",
    "    attempted_move = (np.abs(np.where(requested_moves[:,0]<998.0, requested_moves[:,0]-db['pos_t'], 0.0))<0.01) & \\\n",
    "        (np.abs(np.where(requested_moves[:,1]<998.0, requested_moves[:,1]-db['pos_p'], 0.0))<0.01)\n",
    "    \n",
    "    # Look for interesting phrases in the logs\n",
    "    # Make an array for lines containing handle_fvc_feedback and not containing reject\n",
    "    fvc_feedback = db['ctrl_enabled'] & np.array(list(map(lambda s: (s.find('handle_fvc_feedback')>=0), db['log_note']))) \\\n",
    "        & np.array(list(map(lambda s: (s.find('reject')<0), db['log_note'])))\n",
    "    interfere = np.array(list(map(lambda s: (s.find('interferes')>=0), db['log_note'])))\n",
    "    debounced = np.array(list(map(lambda s: (s.find('Debounced')>=0), db['log_note'])))\n",
    "    debounced_fail = np.array(list(map(lambda s: (s.find('debouncing polygons')>=0), db['log_note'])))\n",
    "    frozen = np.array(list(map(lambda s: (s.find('freeze')>=0), db['log_note']))) & db['ctrl_enabled'] & ~attempted_move\n",
    "    frozen_unflagged = np.array(list(map(lambda s: (s.find('freeze')>=0), db['log_note']))) & (flags<2**16) & (db_iter==0) & ~attempted_move\n",
    "    denied = np.array(list(map(lambda s: (s.find('denied')>=0), db['log_note'])))\n",
    "    autodisabled = np.array(list(map(lambda s: (s.find('Auto-disabling due to bad match')>=0), db['log_note'])))\n",
    "\n",
    "    # New section - AEE\n",
    "    # How many moves happened, according to the measured positions?\n",
    "    sel = (obs_x!=0.0)|(obs_y!=0.0)&(exposure>0)\n",
    "    dx = obs_x[sel][:-1]-obs_x[sel][1:]\n",
    "    dy = obs_y[sel][:-1]-obs_y[sel][1:]\n",
    "    moved = ((dx**2+dy**2)>0.05**2)\n",
    "    Nmoved = np.sum(moved)\n",
    "    if not quiet: print(\"Moved on the following exposures:\", exposure[sel][:-1][moved])\n",
    "\n",
    "    # New section - AEE\n",
    "    # Look for mentions of other fibers\n",
    "    pos_prog = re.compile('(M0\\d\\d\\d\\d)')\n",
    "    def find_pos(log):\n",
    "        match = pos_prog.search(log)\n",
    "        if match: return match.groups()\n",
    "    all_mentions = list(map(find_pos, db['log_note']))\n",
    "    all_mentions = [item for i in all_mentions if i for item in i]\n",
    "    mentions = np.unique(all_mentions)\n",
    "    mentions = ' '.join(mentions)\n",
    "    if not quiet: print(\"Other fibers mentioned in the log\", mentions)\n",
    "    \n",
    "    # Now count the number of certain cases\n",
    "    Nlines = len(db['ctrl_enabled'])\n",
    "    Ndebounced = np.sum(debounced)\n",
    "    Ndebounced_fail = np.sum(debounced_fail)\n",
    "    Nautodisabled = np.sum(autodisabled)\n",
    "    # New line - AEE\n",
    "    Nenabled = np.sum(db['ctrl_enabled'])\n",
    "\n",
    "    Ninterfere = np.sum(interfere)\n",
    "    Nfrozen = np.sum(frozen)\n",
    "    Nfrozen_unflagged = np.sum(frozen_unflagged)\n",
    "    Nmoves = np.sum(move_attempted)\n",
    "    Nbigmoves = np.sum(bigmove)\n",
    "    \n",
    "    # If a move yields a handle_fvc_feedback and the next move is denied and the previous move wasn't, that\n",
    "    # may indicate that the fiber has come into contact with a neighbor, throwing off its\n",
    "    # position.\n",
    "    Nlodged = np.sum(fvc_feedback[1:-1]&denied[:-2]&denied[2:])\n",
    "\n",
    "    if not quiet: \n",
    "        print(\"Returned\", Nlines, \"lines from the DB.\")\n",
    "        print(\"Found\", Ndebounced,\"cases of Debouncing.\", Ninterfere,\"cases of interference.\", Nfrozen,\"cases of being frozen.\")\n",
    "        print(Nmoves, \"moves attempted.  Big moves:\", Nbigmoves)\n",
    "        #print(np.sum(fvc_feedback), \"instances of handle_fvc_feedback\")\n",
    "\n",
    "    # We're concerned when fvc_feedback happens and the previous entry attempted a big move.\n",
    "    # Alas, these vectors are 1 shorter than the original set.\n",
    "    fvc_update = fvc_feedback[:-1] & bigmove[1:]\n",
    "    moved_t = db['pos_t'][:-1]-(db['pos_t'][1:]-move_t[1:])\n",
    "    moved_p = db['pos_p'][:-1]-(db['pos_p'][1:]-move_p[1:])\n",
    "    scale_t = moved_t/(move_t[1:]+1e-10)\n",
    "    scale_p = moved_p/(move_p[1:]+1e-10)\n",
    "\n",
    "\n",
    "    fvc_noupdate = ~fvc_feedback[:-1] & bigmove[1:]\n",
    "    Nbigmoves_ok = np.sum(fvc_noupdate)\n",
    "    Nbigmoves_up = np.sum(fvc_update)\n",
    "\n",
    "    bad_move_t = fvc_update & bigmove_t[1:] & (np.abs(scale_t-1.0)>0.1)\n",
    "    bad_move_p = fvc_update & bigmove_p[1:] & (np.abs(scale_p-1.0)>0.1)\n",
    "\n",
    "    Nbad_t = np.sum(bad_move_t)\n",
    "    Nbad_p = np.sum(bad_move_p)\n",
    "    if not quiet:\n",
    "        print(Nbigmoves_ok, \"big moves were considered ok by FVC.\")\n",
    "        print(Nbigmoves_up, \"big moves were followed by FVC update and usable for scale measurements.\")\n",
    "        print(\"Found\", Nbad_t, \"theta and\", Nbad_p,\"phi bad moves with a scale factor more than 10% off\")\n",
    "        # This block is new - AEE\n",
    "        try: last_bad_theta_date = db['time_recorded'][1:][bad_move_t][-1].isoformat()[0:10]\n",
    "        except: last_bad_theta_date = ''\n",
    "        print(\"Bad theta exposures:\", exposure[1:][bad_move_t], last_bad_theta_date)\n",
    "        try: last_bad_phi_date = db['time_recorded'][1:][bad_move_p][-1].isoformat()[0:10]\n",
    "        except: last_bad_phi_date = ''\n",
    "        print(\"Bad phi exposures:\", exposure[1:][bad_move_p], last_bad_phi_date)\n",
    "\n",
    "    tmp = scale_t[fvc_update&bigmove_t[1:]]\n",
    "#     if not quiet:\n",
    "#         print(\"moved_t\", moved_t[fvc_update&bigmove_t[1:]])\n",
    "#         print(\"move_t\", move_t[1:][fvc_update&bigmove_t[1:]])\n",
    "    \n",
    "    if (len(tmp)>1):\n",
    "        scale_t_rms = np.std(tmp)\n",
    "        if (len(tmp)>2):\n",
    "            tmp_clip = np.sort(tmp)\n",
    "            if tmp_clip[-1]>1.3: tmp_clip = tmp_clip[0:-1]   # Clip the highest value if big\n",
    "            scale_t_mean = np.mean(tmp_clip)\n",
    "            scale_t_err = np.std(tmp_clip)/np.sqrt(len(tmp_clip)+1)\n",
    "        else:\n",
    "            scale_t_mean = np.mean(tmp)\n",
    "            scale_t_err = np.std(tmp)/np.sqrt(len(tmp)+1)\n",
    "        scale_t_max = np.max(np.abs(tmp-1.0))\n",
    "    else:\n",
    "        scale_t_mean = 0.0\n",
    "        scale_t_rms = 0.0\n",
    "        scale_t_err = 0.0\n",
    "        scale_t_max = 0.0\n",
    "\n",
    "    if not quiet: \n",
    "        print(\"Theta:\", f'{scale_t_mean:6.4f}', \"+-\", f'{scale_t_rms:6.4f}',\" max\", scale_t_max)\n",
    "        print(tmp)\n",
    "\n",
    "    tmp = scale_p[fvc_update&bigmove_p[1:]]\n",
    "    if (len(tmp)>1):\n",
    "        scale_p_rms = np.std(tmp)\n",
    "        if (len(tmp)>2):\n",
    "            tmp_clip = np.sort(tmp)\n",
    "            if tmp_clip[-1]>1.3: tmp_clip = tmp_clip[0:-1]   # Clip the highest value if big\n",
    "            scale_p_mean = np.mean(tmp_clip)\n",
    "            scale_p_err = np.std(tmp_clip)/np.sqrt(len(tmp_clip)+1)\n",
    "        else:\n",
    "            scale_p_mean = np.mean(tmp)\n",
    "            scale_p_err = np.std(tmp)/np.sqrt(len(tmp)+1)\n",
    "        scale_p_max = np.max(np.abs(tmp-1.0))\n",
    "    else:\n",
    "        scale_p_mean = 0.0\n",
    "        scale_p_rms = 0.0    \n",
    "        scale_p_err = 0.0    \n",
    "        scale_p_max = 0.0\n",
    "    if not quiet: \n",
    "        print(\"Phi:  \", f'{scale_p_mean:6.4f}', \"+-\", f'{scale_p_rms:6.4f}',\" max\", scale_p_max)\n",
    "        print(tmp)\n",
    "\n",
    "    # We can also look for cases where fvc_feedback keeps returning the same value.\n",
    "    # These may be cases pinned against hardstops, for instance.\n",
    "    # TODO: This requires all cases in a night to be within 1 deg rms.  But if we drive into \n",
    "    # such a point in the middle of the night, we might prefer to ask if the last 3-5 measurements\n",
    "    # are all too close.\n",
    "    hard_theta = 0.0\n",
    "    hard_phi = 0.0\n",
    "    if (np.sum(fvc_update)>3):\n",
    "        tmp = db['pos_t'][:-1][fvc_update][:4]\n",
    "        if (len(tmp)>2)&(np.std(tmp)<1): hard_theta = np.mean(tmp)\n",
    "        tmp = db['pos_p'][:-1][fvc_update][:4]\n",
    "        if (len(tmp)>2)&(np.std(tmp)<1): hard_phi = np.mean(tmp)        \n",
    "\n",
    "    # The 4-digit Location is a helpful way to match to the coordinate files\n",
    "    location = petal_id2loc[db['petal_id'][0]]*1000+db['device_loc']\n",
    "    \n",
    "    # This block is updated in multiple ways - AEE\n",
    "    if not quiet and showlog: \n",
    "        redacted = db[('exposure_id','exposure_iter','pos_t','pos_p','log_note')]\n",
    "        redacted['pos_t'].info.format = '7.3f'\n",
    "        redacted['pos_p'].info.format = '6.3f'\n",
    "        redacted['exposure_id'].info.name = 'expos'\n",
    "        redacted['exposure_iter'].info.name = 'iter'\n",
    "        redacted['log_note'].info.format = '<s'\n",
    "        print(redacted.pprint_all())\n",
    "        print()\n",
    "    \n",
    "    # This list is slightly longer - AEE\n",
    "    # Now return this in a recarray, suitable for concatenation.\n",
    "    return np.array([(db['pos_id'][0], db['petal_id'][0], location[0], mentions, Nautodisabled, Nlodged, Ndebounced, Ndebounced_fail, Ninterfere, Nfrozen, Nfrozen_unflagged, \\\n",
    "                      Nlines, Nenabled, Nmoves, Nmoved, Nbigmoves, Nbigmoves_ok, Nbigmoves_up, Nbad_t, Nbad_p, hard_theta, scale_t_mean, scale_t_err, scale_t_rms, scale_t_max, hard_phi, scale_p_mean, scale_p_err, scale_p_rms, scale_p_max)], \\\n",
    "                    dtype=[('pos_id','<U6'), \\\n",
    "                          ('petal_id','<i4'), \\\n",
    "                          ('location','<i4'), \\\n",
    "                          ('others','<S24'), \\\n",
    "                          ('Nautodis','i4'), \\\n",
    "                          ('Nlodged','i4'), \\\n",
    "                          ('Ndebounced','i4'), \\\n",
    "                          ('Ndebounced_fail','i4'), \\\n",
    "                          ('Ninterfere','<i4'), \\\n",
    "                          ('Nfrozen','<i4'), \\\n",
    "                          ('Nfrozen_unflag','<i4'), \\\n",
    "                          ('Nlines','<i4'), \\\n",
    "                          ('Nenabled','<i4'), \\\n",
    "                          ('Nmoves','<i4'), \\\n",
    "                          ('Nmoved','<i4'), \\\n",
    "                          ('Nbigmoves','<i4'), \\\n",
    "                          ('Nbigmoves_ok','<i4'), \\\n",
    "                          ('Nbigmoves_up','<i4'), \\\n",
    "                          ('Nbad_t','<i4'), \\\n",
    "                          ('Nbad_p','<i4'), \\\n",
    "                          ('hard_t','<f4'), \\\n",
    "                          ('scale_t_mean','<f4'), \\\n",
    "                          ('scale_t_err','<f4'), \\\n",
    "                          ('scale_t_rms','<f4'), \\\n",
    "                          ('scale_t_max','<f4'), \\\n",
    "                          ('hard_p','<f4'), \\\n",
    "                          ('scale_p_mean','<f4'), \\\n",
    "                          ('scale_p_err','<f4'), \\\n",
    "                          ('scale_p_rms','<f4'), \\\n",
    "                          ('scale_p_max','<f4') \\\n",
    "                          ])\n",
    "    \n",
    "#analyze_fiber(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This block is new, or I'd removed it before - AEE\n",
    "# Display the detailed results for a single fiber\n",
    "def show_one_fiber(pos):\n",
    "    onepos = analysis[(analysis[\"pos_id\"]==pos)]\n",
    "    print(\"Found\", pos, \"on petal\", onepos[\"petal_id\"][0])\n",
    "    db = dbs[onepos[\"petal_id\"][0]]\n",
    "    analyze_fiber(db[(np.array(db['pos_id'])==pos)])\n",
    "    return analysis[(analysis[\"pos_id\"]==pos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how one would ask about a single fiber; note that we need the petal id too.\n",
    "# TODO: might code a search across the petals....\n",
    "#db = dbs[11]\n",
    "#out = analyze_fiber(db[(np.array(db['pos_id'])=='M02866')])\n",
    "#Table(out).show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing petal 4\n",
      "Processing petal 5\n",
      "Processing petal 6\n",
      "Processing petal 3\n",
      "Processing petal 8\n",
      "Processing petal 10\n",
      "Processing petal 11\n",
      "Processing petal 2\n",
      "Processing petal 7\n",
      "Processing petal 9\n",
      "All petals processed\n"
     ]
    }
   ],
   "source": [
    "analysis = []\n",
    "\n",
    "for petal in petal_id2loc.keys():\n",
    "    print(\"Processing petal\", petal)\n",
    "    db = dbs[petal]\n",
    "    poslist = np.unique(db['pos_id'])\n",
    "    for pos in poslist:\n",
    "        analysis.append(analyze_fiber(db[(np.array(db['pos_id'])==pos)], quiet=True))\n",
    "    \n",
    "# Adding this back in - AEE\n",
    "print('All petals processed')\n",
    "\n",
    "analysis = np.concatenate([x for x in analysis])\n",
    "analysis = Table(analysis)\n",
    "# Added by me - AEE\n",
    "date_string = np.datetime64('%s-%s-%s' % (str(date_to_analyze)[0:4], str(date_to_analyze)[4:6], str(date_to_analyze)[6:]))\n",
    "analysis.add_column(np.datetime64(date_string), name='date', index=0)\n",
    "analysis['scale_t_mean'].info.format = '5.3f'\n",
    "analysis['scale_t_rms'].info.format = '5.3f'\n",
    "analysis['scale_p_mean'].info.format = '5.3f'\n",
    "analysis['scale_p_rms'].info.format = '5.3f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis[0:10].show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positioners showing evidence of being stuck at one angle (hardstop?), 0 in total\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "hardstop = (analysis['hard_t']!=0) | (analysis['hard_p']!=0) \n",
    "print(\"Positioners showing evidence of being stuck at one angle (hardstop?),\", np.sum(hardstop), \"in total\")\n",
    "print(list(analysis[hardstop]['pos_id']))\n",
    "# I've changed this to be included in output - AEE\n",
    "#analysis[hardstop].show_in_notebook()\n",
    "analysis.add_column(MaskedColumn(data=hardstop, name='hardstop', dtype=bool, fill_value=False), index=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positioners showing non-unity scale factors, 0 in total\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "bad_theta = (analysis['Nbad_t']>2)&(analysis['scale_t_max']>0.1)&(np.abs(analysis['scale_t_mean']-1)>0.1) \\\n",
    "    &(np.abs(analysis['scale_t_mean']-1)>2.5*np.sqrt(analysis['scale_t_err']**2+0.04**2))\n",
    "bad_phi = (analysis['Nbad_p']>2)&(analysis['scale_p_max']>0.1)&(np.abs(analysis['scale_p_mean']-1)>0.1) \\\n",
    "    &(np.abs(analysis['scale_p_mean']-1)>2.5*np.sqrt(analysis['scale_p_err']**2+0.04**2))\n",
    "\n",
    "bad_scale = (bad_theta | bad_phi) & ~hardstop\n",
    "\n",
    "# This requires at least 3 instances of handle_fvc_feedback on a big move.\n",
    "    \n",
    "print(\"Positioners showing non-unity scale factors,\", np.sum(bad_scale), \"in total\")\n",
    "print(list(analysis[bad_scale]['pos_id']))\n",
    "# Changing this to include analysis in data - AEE\n",
    "#analysis[bad_scale].show_in_notebook()\n",
    "analysis.add_column(MaskedColumn(data=bad_scale, name='bad_scale', dtype=bool, fill_value=False), index=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positioners showing evidence of becoming lodged (FVC feedback, then denied further moves), 0 in total\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "lodged = (analysis['Nlodged']>0) \n",
    "print(\"Positioners showing evidence of becoming lodged (FVC feedback, then denied further moves),\", np.sum(lodged), \"in total\")\n",
    "print(list(analysis[lodged]['pos_id']))\n",
    "# Another bit of including this in the output - AEE\n",
    "#analysis[lodged].show_in_notebook()\n",
    "analysis.add_column(MaskedColumn(data=lodged, name='lodged', dtype=bool, fill_value=False), index=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabled Positioners showing evidence of being bumped, 3 in total\n",
      "['M04013', 'M08299', 'M07924']\n",
      "[2, 2, 2]\n",
      "Running the individual fiber will give the exposure number; then plot the configuration.\n"
     ]
    }
   ],
   "source": [
    "# This section is new - AEE\n",
    "# Todo: Need to figure out how to exclude the initial FPsetup, so that we can be more sensitive\n",
    "disabled_moving = (analysis['Nenabled']==0)&(analysis['Nmoved']>1)\n",
    "print(\"Disabled Positioners showing evidence of being bumped,\", np.sum(disabled_moving), \"in total\")\n",
    "print(list(analysis[disabled_moving]['pos_id']))\n",
    "print(list(analysis[disabled_moving]['Nmoved']))\n",
    "print(\"Running the individual fiber will give the exposure number; then plot the configuration.\")\n",
    "# Adding this to the output - AEE\n",
    "#analysis[disabled_moving].show_in_notebook()\n",
    "analysis.add_column(MaskedColumn(data=disabled_moving, name='disabled_moving', dtype=bool, fill_value=False), index=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positioners that were autodisabled, probably during FP setup, 0 in total\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "autodisabled = (analysis['Nautodis']>1) \n",
    "#  TODO: Consider changing this to >1, since the ==1 case may not be during FP setup.\n",
    "print(\"Positioners that were autodisabled, probably during FP setup,\", np.sum(autodisabled), \"in total\")\n",
    "print(list(analysis[autodisabled]['pos_id']))\n",
    "# Adding out output - AEE\n",
    "# Index is higher because of previous addition\n",
    "#analysis[autodisabled].show_in_notebook()\n",
    "analysis.add_column(MaskedColumn(data=autodisabled, name='autodisabled', dtype=bool, fill_value=False), index=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positioners showing other interferences or debounces, 2 in total\n",
      "['M05680', 'M01859']\n"
     ]
    }
   ],
   "source": [
    "concern = (analysis['Ndebounced']>2) | (analysis['Ninterfere']>2) | (analysis['Ndebounced_fail']>2) \n",
    "concern = concern & ~bad_scale & ~lodged & ~autodisabled\n",
    "print(\"Positioners showing other interferences or debounces,\", np.sum(concern), \"in total\")\n",
    "print(list(analysis[concern]['pos_id']))\n",
    "# Adding to output, incrementing index - AEE\n",
    "#analysis[concern].show_in_notebook()\n",
    "analysis.add_column(MaskedColumn(data=concern, name='concern', dtype=bool, fill_value=False), index=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positioners showing unusual amounts of being otherwise frozen, 10 in total\n",
      "['M05288', 'M03612', 'M01362', 'M02866', 'M06238', 'M05389', 'M07643', 'M07655', 'M07752', 'M07864']\n"
     ]
    }
   ],
   "source": [
    "# The numbers in this cutoff have changed - AEE\n",
    "frozen = (analysis[\"Nfrozen\"]>3)|(analysis[\"Nfrozen_unflag\"]>1)\n",
    "frozen = frozen & ~concern & ~bad_scale & ~lodged\n",
    "print(\"Positioners showing unusual amounts of being otherwise frozen,\", np.sum(frozen), \"in total\")\n",
    "print(list(analysis[frozen]['pos_id']))\n",
    "# Adding to output, incrementing index - AEE\n",
    "#analysis[frozen].show_in_notebook()\n",
    "analysis.add_column(MaskedColumn(data=frozen, name='frozen', dtype=bool, fill_value=False), index=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I commented this out - AEE\n",
    "#show_one_fiber('M01784').show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Code below here is trying to look back over many nights at a single fiber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is new - AEE\n",
    "def get_one_fiber(pos, petal, start, end):\n",
    "    _night = f'{start:08d}'\n",
    "    _night = date(int(_night[0:4]),int(_night[4:6]),int(_night[6:8]))\n",
    "    start = _night\n",
    "    _night = f'{end:08d}'\n",
    "    _night = date(int(_night[0:4]),int(_night[4:6]),int(_night[6:8]))\n",
    "    end = _night\n",
    "    print(\"Searching DB from\", start.isoformat(), 'to', end.isoformat())\n",
    "    cmd = \"select * from posmovedb.positioner_moves_p\"+f'{petal:1d}'+\" where (ctrl_enabled=True) and (pos_id = '\"+pos+\"') and (time_recorded > '\"+start.isoformat()+\"') and (time_recorded < '\"+end.isoformat()+\"') order by time_recorded desc\"\n",
    "    print(cmd)\n",
    "    db=dbquery(comm,cmd)\n",
    "    db = Table(db)\n",
    "    print(len(db), \"entries retrieved\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fib = get_one_fiber('M04915', 7, 20210410, 20210413)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze_fiber(fib,quiet=False, showlog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to DB complete\n",
      "This took 67.188812 seconds\n"
     ]
    }
   ],
   "source": [
    "# This is all mine - AEE\n",
    "if write_to_file:\n",
    "    output_to_file(analysis, '%i_analysis.json' % date_to_analyze, format='pandas.json')\n",
    "    output_to_file(analysis, '%i_analysis.csv' % date_to_analyze)\n",
    "if write_to_db:\n",
    "    # This is a real pain to do with numpy, but is a one liner on a dataframe\n",
    "    from pandas import DataFrame\n",
    "    from sqlalchemy import create_engine\n",
    "    import urllib\n",
    "    pandas_analysis = DataFrame(data=[row for row in analysis], columns=[c.lower() for c in list(analysis.columns)])\n",
    "    # Ok! To the database!\n",
    "    writer_args = {'host': os.getenv('DOS_DB_HOST'),\n",
    "                   'port': int(os.getenv('DOS_DB_PORT')),\n",
    "                   'database': os.getenv('DOS_DB_NAME'),\n",
    "                   'user': os.getenv('DOS_DB_WRITER'),\n",
    "                   'password': urllib.parse.quote_plus(os.getenv('DOS_DB_WRITER_PASSWORD'))}\n",
    "    writer_engine = create_engine('postgresql://{user}:{password}@{host}:{port}/{database}'.format(**writer_args))\n",
    "    #help(pandas_analysis.to_sql)\n",
    "    pandas_analysis.to_sql(name='fiber_analysis', con=writer_engine, schema='telemetry', if_exists='append', index=False)\n",
    "    print('Writing to DB complete')\n",
    "end_time = time.time()\n",
    "print('This took %f seconds' % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'call_nonsense_to_halt_the_kernel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-97b006b6a42b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# I'm not sure why this is here? Is this instead of a sys.exit() that would end a script? - AEE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcall_nonsense_to_halt_the_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'call_nonsense_to_halt_the_kernel' is not defined"
     ]
    }
   ],
   "source": [
    "# I'm not sure why this is here? Is this instead of a sys.exit() that would end a script? - AEE\n",
    "call_nonsense_to_halt_the_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "Code below here is looking at the calibration database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calibDB():\n",
    "    cmd = \"select t.* from posmovedb.positioner_calibration t inner join ( select pos_id, max(time_recorded) as MaxDate from posmovedb.positioner_calibration group by pos_id ) tm on t.pos_id = tm.pos_id and t.time_recorded = tm.MaxDate\"\n",
    "    print(cmd)\n",
    "    calib=dbquery(comm,cmd)\n",
    "    calib = Table(calib)\n",
    "    print(len(calib), \"entries retrieved\")\n",
    "    return calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib = get_calibDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib[:1].show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calib.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buses = np.unique(calib['bus_id'])\n",
    "for bus in buses:\n",
    "    sel = (calib['bus_id']==bus)\n",
    "    plt.scatter(calib['offset_x'][sel], calib['offset_y'][sel], s=12)\n",
    "\n",
    "plt.title(\"Distinct CANbus Regions on a Petal\", size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_phi = (calib[\"gear_calib_p\"]<1)\n",
    "linear_theta = (calib[\"gear_calib_t\"]<1)\n",
    "\n",
    "plt.hist(calib[\"offset_p\"],bins=70,range=[-20,50])\n",
    "plt.hist(calib[\"offset_p\"][linear_phi],bins=70,range=[-20,50],color='r')\n",
    "plt.xlabel(\"Offset_p\", size=14)\n",
    "plt.ylabel(\"Histogram (note log-scale)\", size=14)\n",
    "plt.text(15, 800, \"Red: gear_calib_p<1\", size=14)\n",
    "plt.text(20, 400, \"(aka 'linear-phi')\", size=14)\n",
    "plt.semilogy()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "Looking at the calibrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = Table.read(\"/global/u1/e/eisenste/Calibration/20201217/phi_theta_fits.csv\")\n",
    "dx = cal['phi_x']-cal['theta_x']\n",
    "dy = cal['phi_y']-cal['theta_y']\n",
    "theta_arm = np.sqrt(dx*dx+dy*dy)\n",
    "good = (cal['phi_r']>0)&(cal['theta_r']>0)&(cal['phi_r']<3.5)\n",
    "print(len(good), np.sum(good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median(cal['phi_r']), np.max(cal['phi_r']))\n",
    "print(np.quantile(cal['phi_r'][good],[0.05,0.1,0.25,0.5,0.75,0.9,0.95]))\n",
    "\n",
    "\n",
    "print(np.quantile(theta_arm[good],[0.05,0.1,0.25,0.5,0.75,0.9,0.95]))\n",
    "print(np.sort(theta_arm[good]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "files = glob(\"/global/u1/e/eisenste/Calibration/20201217/dat/M*_paramfits.csv\")\n",
    "tabs = []\n",
    "for file in files:\n",
    "    tabs.append(Table.read(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.table\n",
    "params = astropy.table.vstack(tabs, join_type='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = (params['NUM_POINTS']>30)&(params['ERR.LENGTH_R1_STATIC']<0.5)&(params['ERR.LENGTH_R2_STATIC']<0.5)\n",
    "params[sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = (params['POS_ID']=='M06185')\n",
    "for k in params.keys():\n",
    "    if k[:2]=='ER' and k[-3]=='T': print(k, float(params[sel][k]))\n",
    "params[sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = params[((params['POS_ID']=='M06185'))]\n",
    "#param = params[((params['POS_ID']=='M07403'))]\n",
    "\n",
    "\n",
    "#param = Table.read(\"/global/u1/e/eisenste/Calibration/20201217/M07407_paramfits.csv\")\n",
    "\n",
    "\n",
    "par = ['LENGTH_R1_STATIC', 'LENGTH_R2_STATIC', 'OFFSET_X_STATIC', 'OFFSET_Y_STATIC', 'OFFSET_T_STATIC', 'OFFSET_P_STATIC']\n",
    "for p in par:\n",
    "    print(p, float(param[p]), float(param['ERR.'+p]) )\n",
    "    \n",
    "cov = np.zeros([len(par),len(par)])\n",
    "for i,p in enumerate(par):\n",
    "    for j,q in enumerate(par):\n",
    "        if i==j: cov[i][j] = param['ERR.'+p]**2\n",
    "        else:\n",
    "            try:\n",
    "                cov[i][j] = param['COV.'+p+'.'+q]\n",
    "            except:\n",
    "                cov[i][j] = param['COV.'+q+'.'+p]\n",
    "            cov[j][i] = cov[i][j]\n",
    "#print(cov)\n",
    "isq = 1/np.sqrt(np.diag(cov))   \n",
    "rcov = ((cov*isq).T*isq).T\n",
    "print(rcov)\n",
    "print(scipy.linalg.eigh(rcov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
